{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f2244f11eab922",
   "metadata": {},
   "source": [
    "# Qdrant indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d321e6755b15e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T09:34:55.653622Z",
     "start_time": "2024-07-15T09:34:46.580296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '4983',\n",
       " 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.',\n",
       " 'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"BeIR/scifact\", \"corpus\", split=\"corpus\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26797f7a57adc4ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T09:34:55.657745Z",
     "start_time": "2024-07-15T09:34:55.654856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5183"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976c453bf0d27c0",
   "metadata": {},
   "source": [
    "## Dense embeddings\n",
    "\n",
    "We're not going to choose the fanciest embedding model out there, but stick to something simple and efficient. FastEmbed comes with some pretrained models that we can use out of the box. Due to ONNX usage, these models can be launched efficiently even on a CPU. The `all-MiniLM-L6-v2` model is a lightweight model that's good for a start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611ad98114d793b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T09:47:53.926605Z",
     "start_time": "2024-07-15T09:42:24.389900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-18 17:10:50.908\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mfastembed.embedding\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mDefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated.Use from fastembed import TextEmbedding instead.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92583f75dd5f441680e3a572c87565b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed.embedding import TextEmbedding\n",
    "\n",
    "dense_embedding_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "dense_embeddings = list(dense_embedding_model.passage_embed(dataset[\"text\"][0:1]))\n",
    "len(dense_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702cbdb3d2c6a32a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T11:30:49.594058Z",
     "start_time": "2024-07-15T11:30:49.587212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dense_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74386f75bb96941",
   "metadata": {},
   "source": [
    "## Sparse embeddings\n",
    "\n",
    "Similarly, we can use a BM25 model to generate sparse embeddings, so it hopefully will handle the cases in which the dense embeddings fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70bc60aa2d0956d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T09:58:19.415397Z",
     "start_time": "2024-07-15T09:58:15.358148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc98341ee0749e9806c9711f97c7656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed.sparse.bm25 import Bm25\n",
    "\n",
    "bm25_embedding_model = Bm25(\"Qdrant/bm25\")\n",
    "bm25_embeddings = list(bm25_embedding_model.passage_embed(dataset[\"text\"][0:1]))\n",
    "len(bm25_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443ae4affae64c6",
   "metadata": {},
   "source": [
    "## Late interaction embeddings\n",
    "\n",
    "Both dense and sparse embeddings we generated so far produce a single embedding per document. However, there is also a different approach, where we can generate a single embedding per token. For each query token we take its maximum similarity against the document and sum across query terms. This kind of models is called late interaction models, and ColBERT is one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a177b683a57aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T10:41:37.087598Z",
     "start_time": "2024-07-15T10:12:33.013136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37f214477b34f2abafc809dc1255928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed.late_interaction import LateInteractionTextEmbedding\n",
    "\n",
    "late_interaction_embedding_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n",
    "late_interaction_embeddings = list(late_interaction_embedding_model.passage_embed(dataset[\"text\"][0:1]))\n",
    "len(late_interaction_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e0690fdd4c36d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T11:30:39.061309Z",
     "start_time": "2024-07-15T11:30:39.056798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(late_interaction_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fbac945e35a55",
   "metadata": {},
   "source": [
    "## Putting data in a Qdrant collection\n",
    "\n",
    "All the vectors might be now upserted into a Qdrant collection. Keeping them all in a single one enables the possibility to combine different embeddings and create even a complex pipeline with several steps. Depending on the specifics of your data, you may prefer to use a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431ec613-ad0c-4a35-b0da-bb2f69294999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4cea045985b0ea2b1f1785a9f3156f49d404f3d387e4919205ad59c0a05854cc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:v1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0ba5b8b767ae5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T11:35:45.875085Z",
     "start_time": "2024-07-15T11:35:45.785286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\", timeout=600)\n",
    "client.create_collection(\n",
    "    \"scifact\",\n",
    "    vectors_config={\n",
    "        \"all-MiniLM-L6-v2\": models.VectorParams(\n",
    "            size=len(dense_embeddings[0]),\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "        \"colbertv2.0\": models.VectorParams(\n",
    "            size=len(late_interaction_embeddings[0][0]),\n",
    "            distance=models.Distance.COSINE,\n",
    "            multivector_config=models.MultiVectorConfig(\n",
    "                comparator=models.MultiVectorComparator.MAX_SIM,\n",
    "            )\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238650e7e3ea136e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-15T11:53:37.675152Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                                                                                                                                             | 7/1295 [00:17<53:55,  2.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m dense_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dense_embedding_model\u001b[38;5;241m.\u001b[39mpassage_embed(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m      7\u001b[0m bm25_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(bm25_embedding_model\u001b[38;5;241m.\u001b[39mpassage_embed(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m----> 8\u001b[0m late_interaction_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlate_interaction_embedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpassage_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m client\u001b[38;5;241m.\u001b[39mupload_points(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscifact\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     points\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,  \n\u001b[1;32m     32\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ultimate-hybrid-search-workshop-gQON67yw-py3.10/lib/python3.10/site-packages/fastembed/late_interaction/late_interaction_embedding_base.py:43\u001b[0m, in \u001b[0;36mLateInteractionTextEmbeddingBase.passage_embed\u001b[0;34m(self, texts, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mEmbeds a list of text passages into a list of embeddings.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Iterable[np.ndarray]: The embeddings.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This is model-specific, so that different models can have specialized implementations\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(texts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ultimate-hybrid-search-workshop-gQON67yw-py3.10/lib/python3.10/site-packages/fastembed/late_interaction/late_interaction_text_embedding.py:93\u001b[0m, in \u001b[0;36mLateInteractionTextEmbedding.embed\u001b[0;34m(self, documents, batch_size, parallel, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     73\u001b[0m     documents: Union[\u001b[38;5;28mstr\u001b[39m, Iterable[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     77\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    Encode a list of documents into list of embeddings.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    We use mean pooling with attention so that the model can handle variable-length inputs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m        List of embeddings, one per document\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed(documents, batch_size, parallel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ultimate-hybrid-search-workshop-gQON67yw-py3.10/lib/python3.10/site-packages/fastembed/late_interaction/colbert.py:169\u001b[0m, in \u001b[0;36mColbert.embed\u001b[0;34m(self, documents, batch_size, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     documents: Union[\u001b[38;5;28mstr\u001b[39m, Iterable[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    153\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    Encode a list of documents into list of embeddings.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    We use mean pooling with attention so that the model can handle variable-length inputs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m        List of embeddings, one per document\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_documents(\n\u001b[1;32m    170\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m    171\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir),\n\u001b[1;32m    172\u001b[0m         documents\u001b[38;5;241m=\u001b[39mdocuments,\n\u001b[1;32m    173\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    174\u001b[0m         parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    176\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ultimate-hybrid-search-workshop-gQON67yw-py3.10/lib/python3.10/site-packages/fastembed/text/onnx_text_model.py:109\u001b[0m, in \u001b[0;36mOnnxTextModel._embed_documents\u001b[0;34m(self, model_name, cache_dir, documents, batch_size, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m is_small:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(documents, batch_size):\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_onnx_output(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     start_method \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforkserver\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforkserver\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m get_all_start_methods() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ultimate-hybrid-search-workshop-gQON67yw-py3.10/lib/python3.10/site-packages/fastembed/text/onnx_text_model.py:78\u001b[0m, in \u001b[0;36mOnnxTextModel.onnx_embed\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     onnx_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m     73\u001b[0m         [np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(e), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m input_ids], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     76\u001b[0m onnx_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_onnx_input(onnx_input, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 78\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mONNX_OUTPUT_NAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OnnxOutputContext(\n\u001b[1;32m     80\u001b[0m     model_output\u001b[38;5;241m=\u001b[39mmodel_output[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     81\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39monnx_input\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, attention_mask),\n\u001b[1;32m     82\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39monnx_input\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_ids),\n\u001b[1;32m     83\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ultimate-hybrid-search-workshop-gQON67yw-py3.10/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "batch_size = 4\n",
    "for batch in tqdm.tqdm(dataset.iter(batch_size=batch_size), \n",
    "                       total=len(dataset) // batch_size):\n",
    "    dense_embeddings = list(dense_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    bm25_embeddings = list(bm25_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    late_interaction_embeddings = list(late_interaction_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    \n",
    "    client.upload_points(\n",
    "        \"scifact\",\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=int(batch[\"_id\"][i]),\n",
    "                vector={\n",
    "                    \"all-MiniLM-L6-v2\": dense_embeddings[i].tolist(),\n",
    "                    \"bm25\": bm25_embeddings[i].as_object(),\n",
    "                    \"colbertv2.0\": late_interaction_embeddings[i].tolist(),\n",
    "                },\n",
    "                payload={\n",
    "                    \"_id\": batch[\"_id\"][i],\n",
    "                    \"title\": batch[\"title\"][i],\n",
    "                    \"text\": batch[\"text\"][i],\n",
    "                }\n",
    "            )\n",
    "            for i, _ in enumerate(batch[\"_id\"])\n",
    "        ],\n",
    "        # We send a lot of embeddings at once, so it's best to reduce the batch size.\n",
    "        # Otherwise, we would have gigantic requests sent for each batch and we can\n",
    "        # easily reach the maximum size of a single request.\n",
    "        batch_size=batch_size,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ae256b5-b874-41e4-b633-85c16f29f2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.recover_snapshot(\n",
    "    \"scifact\",\n",
    "    location=\"https://storage.googleapis.com/common-datasets-snapshots/scifact-multiple-representations.snapshot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d8154fe8e865a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=None, indexed_vectors_count=15549, points_count=5183, segments_count=7, config=CollectionConfig(params=CollectionParams(vectors={'all-MiniLM-L6-v2': VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), 'colbertv2.0': VectorParams(size=128, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=MultiVectorConfig(comparator=<MultiVectorComparator.MAX_SIM: 'max_sim'>))}, shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors={'bm25': SparseVectorParams(index=None, modifier=<Modifier.IDF: 'idf'>)}), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_collection(\"scifact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84203d-8ecb-4d8e-bd00-a18ec524cf52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
